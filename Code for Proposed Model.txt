BanglaBERT with Attention:-------------------------------------------------------------------------------------------------------------

import time
import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

!pip install transformers datasets

from transformers import (
    AutoTokenizer, AutoModel, Trainer, TrainingArguments,
    DataCollatorWithPadding
)
from datasets import Dataset, DatasetDict

# Set seed for reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

# Load tokenizer and model name
model_name = "csebuetnlp/banglabert"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load dataset
file_path = "/content/ALERT.xlsx"
df = pd.read_excel(file_path)
print("Unique labels:", df['Final Annotation'].unique())

# Convert labels to numeric format
label_mapping = {"no aggression": 3, "atrocity": 2, "vandalism": 1, "hate": 0}
df['label'] = df['Final Annotation'].map(label_mapping)

# Split dataset
train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=seed)
validation_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=seed)

# Convert DataFrame to Hugging Face Datasets
hf_train = Dataset.from_pandas(train_df)
hf_valid = Dataset.from_pandas(validation_df)
hf_test = Dataset.from_pandas(test_df)

data = DatasetDict({"train": hf_train, "validation": hf_valid})

# Tokenization function
def tokenize(batch):
    return tokenizer(batch['Text'], padding="max_length", truncation=True, max_length=512)

data_encoded = data.map(tokenize, batched=True, batch_size=16)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Define the model with attention
class BanglaBERTWithAttention(nn.Module):
    def __init__(self, model_name, num_labels, dropout_rate=0.2):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.bert.config.hidden_size
        self.attention = nn.Linear(self.hidden_size, 1)
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        weights = torch.softmax(torch.tanh(self.attention(last_hidden_state)), dim=1)
        context = torch.sum(weights * last_hidden_state, dim=1)
        logits = self.classifier(self.dropout(context))
        loss = F.cross_entropy(logits, labels) if labels is not None else None
        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

# Initialize model
model = BanglaBERTWithAttention(model_name=model_name, num_labels=4).to("cuda")

# Metric computation
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=-1)
    return {
        "Accuracy": accuracy_score(labels, preds),
        "F1 Score": f1_score(labels, preds, average="weighted")
    }


import transformers
!pip install --upgrade transformers
from transformers import TrainingArguments
print(transformers.__version__)

training_args = TrainingArguments(
    output_dir=f"{model_name}-classifier",         # Output directory
    overwrite_output_dir=True,                     # Overwrite if exists
    #evaluation_strategy="epoch",   # ✅ REQUIRED to get validation logs
    #logging_strategy="epoch",      # ✅ Optional: match logging with eval
    do_train=True,                                 # Enable training
    do_eval=True,                                  # Enable evaluation
    per_device_train_batch_size=8,                 # Batch size for training
    per_device_eval_batch_size=8,                  # Batch size for evaluation
    num_train_epochs=10,                           # Total number of training epochs
    learning_rate=5e-6,                            # Learning rate
    weight_decay=0.01,                             # Weight decay
    logging_dir=f"{model_name}-logs",              # Directory for logs
    logging_steps=len(data_encoded['train']) // 8, # Log every N steps
    save_steps=len(data_encoded['train']) // 8,    # Save every N steps
    save_total_limit=1,                            # Keep only last checkpoint
    seed=42,                                        # Reproducibility
    report_to="none"
)


# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data_encoded["train"],
    eval_dataset=data_encoded["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train model
start_time = time.time()
trainer.train()
print(f"Training time: {time.time() - start_time:.2f} seconds")

# Extract training history
log_history = trainer.state.log_history

# Extract metrics
epochs, train_loss, eval_loss, train_acc, eval_acc = [], [], [], [], []
for entry in log_history:
    if "epoch" in entry:
        epochs.append(entry["epoch"])
        train_loss.append(entry.get("loss"))
        eval_loss.append(entry.get("eval_loss"))
        train_acc.append(entry.get("accuracy"))
        eval_acc.append(entry.get("eval_Accuracy"))

# Plot loss curve
plt.figure(figsize=(8, 5))
plt.plot(epochs[:len(train_loss)], train_loss, label="Training Loss", marker="o")
plt.plot(epochs[:len(eval_loss)], eval_loss, label="Validation Loss", marker="s")
plt.title("Loss vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Save model
torch.save(model.state_dict(), "banglabert_trained_model.pth")
tokenizer.save_pretrained("banglabert_trained_tokenizer")

# Evaluation
validation_metrics = trainer.evaluate()
print("Validation Results:", validation_metrics)

# Encode and evaluate on test set
hf_test_encoded = hf_test.map(tokenize, batched=True)
test_results = trainer.predict(hf_test_encoded)
test_metrics = compute_metrics(test_results)
print("Test Results:", test_metrics)

# Save predictions
y_true = test_results.label_ids
y_logits = test_results.predictions
y_prob = F.softmax(torch.tensor(y_logits), dim=1).numpy()

preds_df = pd.DataFrame({
    "y_true": y_true,
    **{f"model_1_prob_class_{i}": y_prob[:, i] for i in range(y_prob.shape[1])}
})

preds_df.to_csv("banglabert_attention_predictions.csv", index=False)
print("Predictions saved!")



XLM RoBERTa with Attention:------------------------------------------------------------------------------------------------------------


import time
import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

!pip install transformers datasets

from transformers import (
    AutoTokenizer, AutoModel, Trainer, TrainingArguments,
    DataCollatorWithPadding
)
from datasets import Dataset, DatasetDict

# Set seed for reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

# Load tokenizer and model name
model_name = "csebuetnlp/banglabert"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load dataset
file_path = "/content/ALERT.xlsx"
df = pd.read_excel(file_path)
print("Unique labels:", df['Final Annotation'].unique())

# Convert labels to numeric format
label_mapping = {"no aggression": 3, "atrocity": 2, "vandalism": 1, "hate": 0}
df['label'] = df['Final Annotation'].map(label_mapping)

# Split dataset
train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=seed)
validation_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=seed)

# Convert DataFrame to Hugging Face Datasets
hf_train = Dataset.from_pandas(train_df)
hf_valid = Dataset.from_pandas(validation_df)
hf_test = Dataset.from_pandas(test_df)

data = DatasetDict({"train": hf_train, "validation": hf_valid})

# Tokenization function
def tokenize(batch):
    return tokenizer(batch['Text'], padding="max_length", truncation=True, max_length=512)

data_encoded = data.map(tokenize, batched=True, batch_size=16)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Define the model with attention
class BanglaBERTWithAttention(nn.Module):
    def __init__(self, model_name, num_labels, dropout_rate=0.2):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.bert.config.hidden_size
        self.attention = nn.Linear(self.hidden_size, 1)
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        weights = torch.softmax(torch.tanh(self.attention(last_hidden_state)), dim=1)
        context = torch.sum(weights * last_hidden_state, dim=1)
        logits = self.classifier(self.dropout(context))
        loss = F.cross_entropy(logits, labels) if labels is not None else None
        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

# Initialize model
model = BanglaBERTWithAttention(model_name=model_name, num_labels=4).to("cuda")

# Metric computation
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=-1)
    return {
        "Accuracy": accuracy_score(labels, preds),
        "F1 Score": f1_score(labels, preds, average="weighted")
    }


import transformers
!pip install --upgrade transformers
from transformers import TrainingArguments
print(transformers.__version__)

training_args = TrainingArguments(
    output_dir=f"{model_name}-classifier",         # Output directory
    overwrite_output_dir=True,                     # Overwrite if exists
    #evaluation_strategy="epoch",   # ✅ REQUIRED to get validation logs
    #logging_strategy="epoch",      # ✅ Optional: match logging with eval
    do_train=True,                                 # Enable training
    do_eval=True,                                  # Enable evaluation
    per_device_train_batch_size=8,                 # Batch size for training
    per_device_eval_batch_size=8,                  # Batch size for evaluation
    num_train_epochs=10,                           # Total number of training epochs
    learning_rate=5e-6,                            # Learning rate
    weight_decay=0.01,                             # Weight decay
    logging_dir=f"{model_name}-logs",              # Directory for logs
    logging_steps=len(data_encoded['train']) // 8, # Log every N steps
    save_steps=len(data_encoded['train']) // 8,    # Save every N steps
    save_total_limit=1,                            # Keep only last checkpoint
    seed=42,                                        # Reproducibility
    report_to="none"
)


# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data_encoded["train"],
    eval_dataset=data_encoded["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train model
start_time = time.time()
trainer.train()
print(f"Training time: {time.time() - start_time:.2f} seconds")

# Extract training history
log_history = trainer.state.log_history

# Extract metrics
epochs, train_loss, eval_loss, train_acc, eval_acc = [], [], [], [], []
for entry in log_history:
    if "epoch" in entry:
        epochs.append(entry["epoch"])
        train_loss.append(entry.get("loss"))
        eval_loss.append(entry.get("eval_loss"))
        train_acc.append(entry.get("accuracy"))
        eval_acc.append(entry.get("eval_Accuracy"))

# Plot loss curve
plt.figure(figsize=(8, 5))
plt.plot(epochs[:len(train_loss)], train_loss, label="Training Loss", marker="o")
plt.plot(epochs[:len(eval_loss)], eval_loss, label="Validation Loss", marker="s")
plt.title("Loss vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Save model
torch.save(model.state_dict(), "banglabert_trained_model.pth")
tokenizer.save_pretrained("banglabert_trained_tokenizer")

# Evaluation
validation_metrics = trainer.evaluate()
print("Validation Results:", validation_metrics)

# Encode and evaluate on test set
hf_test_encoded = hf_test.map(tokenize, batched=True)
test_results = trainer.predict(hf_test_encoded)
test_metrics = compute_metrics(test_results)
print("Test Results:", test_metrics)

# Save predictions
y_true = test_results.label_ids
y_logits = test_results.predictions
y_prob = F.softmax(torch.tensor(y_logits), dim=1).numpy()

preds_df = pd.DataFrame({
    "y_true": y_true,
    **{f"model_1_prob_class_{i}": y_prob[:, i] for i in range(y_prob.shape[1])}
})

preds_df.to_csv("banglabert_attention_predictions.csv", index=False)
print("Predictions saved!")



Weighted ensamble code:----------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,
    ConfusionMatrixDisplay, classification_report, roc_curve, auc
)
from sklearn.model_selection import train_test_split

# ✅ Load Predictions from Both Models
  banglabert_df = pd.read_csv("/content/banglabert_attention_predictions.csv")
  xlm_roberta_df = pd.read_csv("/content/xlm_roberta_attention_predictions (1).csv")

# ✅ Ensure both have the same true labels
assert np.array_equal(banglabert_df["y_true"], xlm_roberta_df["y_true"]), "Mismatch in true labels!"

# ✅ Extract True Labels and Probabilities
y_true = banglabert_df["y_true"]

# Extract probabilities for all classes from both models
banglabert_probs = banglabert_df.iloc[:, 1:].values  # Model 1 probabilities
xlm_roberta_probs = xlm_roberta_df.iloc[:, 1:].values  # Model 2 probabilities

#input test accuracies of the models

w_1 = 86.7
w_2 = 85.3

# ✅ Compute Weighted Ensemble Probabilities
ensemble_probs = ((banglabert_probs * w_1) + (xlm_roberta_probs * w_2)) / (w_1 + w_2)

# ✅ Compute Final Predictions
ensemble_preds = np.argmax(ensemble_probs, axis=1)

# ✅ Evaluate Performance
weighted_ensemble_acc = accuracy_score(y_true, ensemble_preds)
weighted_ensemble_f1 = f1_score(y_true, ensemble_preds, average="weighted")
print(f"✅ Weighted Ensemble - Accuracy: {weighted_ensemble_acc:.4f}, F1 Score: {weighted_ensemble_f1:.4f}")

# ✅ Define class labels
class_labels = ["Hate", "Vandalism", "Atrocity", "No Aggression"]

# ✅ Compute Confusion Matrix
cm = confusion_matrix(y_true, ensemble_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)

# ✅ Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap="Greens", xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix of Weighted Ensemble")
plt.show()

# ✅ Generate Classification Report
report = classification_report(y_true, ensemble_preds, target_names=class_labels, digits=4)
print("\nClassification Report:\n", report)

# ✅ Compute Accuracy Per Class
accuracy_per_class = []
for i in range(len(class_labels)):
    class_mask = (y_true == i)
    class_accuracy = accuracy_score(y_true[class_mask], ensemble_preds[class_mask])
    accuracy_per_class.append(class_accuracy)

# ✅ Print Accuracy per Class
print("\nAccuracy Per Class:")
for label, acc in zip(class_labels, accuracy_per_class):
    print(f"{label}: {acc:.4f}")

# ✅ Compute Overall Performance Metrics
precision = precision_score(y_true, ensemble_preds, average='macro')
recall = recall_score(y_true, ensemble_preds, average='macro')
f1 = f1_score(y_true, ensemble_preds, average='macro')
weighted_f1 = f1_score(y_true, ensemble_preds, average='weighted')
error_rate = 1 - weighted_ensemble_acc

print(f"\n✅ Overall Test Accuracy: {weighted_ensemble_acc:.4f}")
print(f"✅ Overall Test Precision: {precision:.4f}")
print(f"✅ Overall Test Recall: {recall:.4f}")
print(f"✅ Overall Test F1 Score: {f1:.4f}")
print(f"✅ Weighted F1 Score: {weighted_f1:.4f}")
print(f"✅ Error Rate: {error_rate:.4f}")

# ✅ Plot ROC Curve
plt.figure(figsize=(7, 5))
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
for i in range(len(class_labels)):
    fpr, tpr, _ = roc_curve(y_true == i, ensemble_probs[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})', linestyle='-', linewidth=2, color=colors[i])

plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Weighted Ensemble")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# ✅ Compute Metrics Per Class
precision_per_class = precision_score(y_true, ensemble_preds, average=None)
recall_per_class = recall_score(y_true, ensemble_preds, average=None)
f1_per_class = f1_score(y_true, ensemble_preds, average=None)

# ✅ Compute Macro and Weighted Averages
macro_avg = [weighted_ensemble_acc, precision, recall, f1]
weighted_avg = [weighted_ensemble_acc, precision_score(y_true, ensemble_preds, average='weighted'), recall_score(y_true, ensemble_preds, average='weighted'), weighted_f1]

# ✅ Create and Display 2D Metrics Matrix
metrics_df = pd.DataFrame(
    data=np.vstack([accuracy_per_class, precision_per_class, recall_per_class, f1_per_class]).T,
    index=class_labels,
    columns=["Accuracy", "Precision", "Recall", "F1 Score"]
)
metrics_df.loc["M. avg"] = macro_avg
metrics_df.loc["W. avg"] = weighted_avg

plt.figure(figsize=(6, 4))
sns.heatmap(metrics_df, annot=True, fmt=".4f", cmap="Greens", linewidths=0.5)
plt.title("Performance Metrics per Class for Weighted Ensemble")
plt.show()