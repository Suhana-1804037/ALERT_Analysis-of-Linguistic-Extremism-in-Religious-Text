{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs66cTvW34Gl"
      },
      "source": [
        "#Banglabert with attention\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GskqPaK24IGJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "!pip install transformers datasets\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, Trainer, TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Load tokenizer and model name\n",
        "model_name = \"csebuetnlp/banglabert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/BOLT.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "print(\"Unique labels:\", df['Final Annotation'].unique())\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_mapping = {\"no aggression\": 3, \"atrocity\": 2, \"vandalism\": 1, \"hate\": 0}\n",
        "df['label'] = df['Final Annotation'].map(label_mapping)\n",
        "\n",
        "# Split dataset\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=seed)\n",
        "validation_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=seed)\n",
        "\n",
        "# Convert DataFrame to Hugging Face Datasets\n",
        "hf_train = Dataset.from_pandas(train_df)\n",
        "hf_valid = Dataset.from_pandas(validation_df)\n",
        "hf_test = Dataset.from_pandas(test_df)\n",
        "\n",
        "data = DatasetDict({\"train\": hf_train, \"validation\": hf_valid})\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['Text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "data_encoded = data.map(tokenize, batched=True, batch_size=16)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define the model with attention\n",
        "class BanglaBERTWithAttention(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.attention = nn.Linear(self.hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        weights = torch.softmax(torch.tanh(self.attention(last_hidden_state)), dim=1)\n",
        "        context = torch.sum(weights * last_hidden_state, dim=1)\n",
        "        logits = self.classifier(self.dropout(context))\n",
        "        loss = F.cross_entropy(logits, labels) if labels is not None else None\n",
        "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
        "\n",
        "# Initialize model\n",
        "model = BanglaBERTWithAttention(model_name=model_name, num_labels=4).to(\"cuda\")\n",
        "\n",
        "# Metric computation\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=-1)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(labels, preds),\n",
        "        \"F1 Score\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "\n",
        "import transformers\n",
        "!pip install --upgrade transformers\n",
        "from transformers import TrainingArguments\n",
        "print(transformers.__version__)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-classifier\",         # Output directory\n",
        "    overwrite_output_dir=True,                     # Overwrite if exists\n",
        "    #evaluation_strategy=\"epoch\",   # ✅ REQUIRED to get validation logs\n",
        "    #logging_strategy=\"epoch\",      # ✅ Optional: match logging with eval\n",
        "    do_train=True,                                 # Enable training\n",
        "    do_eval=True,                                  # Enable evaluation\n",
        "    per_device_train_batch_size=8,                 # Batch size for training\n",
        "    per_device_eval_batch_size=8,                  # Batch size for evaluation\n",
        "    num_train_epochs=10,                           # Total number of training epochs\n",
        "    learning_rate=5e-6,                            # Learning rate\n",
        "    weight_decay=0.01,                             # Weight decay\n",
        "    logging_dir=f\"{model_name}-logs\",              # Directory for logs\n",
        "    logging_steps=len(data_encoded['train']) // 8, # Log every N steps\n",
        "    save_steps=len(data_encoded['train']) // 8,    # Save every N steps\n",
        "    save_total_limit=1,                            # Keep only last checkpoint\n",
        "    seed=42,                                        # Reproducibility\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=data_encoded[\"train\"],\n",
        "    eval_dataset=data_encoded[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Extract training history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract metrics\n",
        "epochs, train_loss, eval_loss, train_acc, eval_acc = [], [], [], [], []\n",
        "for entry in log_history:\n",
        "    if \"epoch\" in entry:\n",
        "        epochs.append(entry[\"epoch\"])\n",
        "        train_loss.append(entry.get(\"loss\"))\n",
        "        eval_loss.append(entry.get(\"eval_loss\"))\n",
        "        train_acc.append(entry.get(\"accuracy\"))\n",
        "        eval_acc.append(entry.get(\"eval_Accuracy\"))\n",
        "\n",
        "# Plot loss curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs[:len(train_loss)], train_loss, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(epochs[:len(eval_loss)], eval_loss, label=\"Validation Loss\", marker=\"s\")\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), \"banglabert_trained_model.pth\")\n",
        "tokenizer.save_pretrained(\"banglabert_trained_tokenizer\")\n",
        "\n",
        "# Evaluation\n",
        "validation_metrics = trainer.evaluate()\n",
        "print(\"Validation Results:\", validation_metrics)\n",
        "\n",
        "# Encode and evaluate on test set\n",
        "hf_test_encoded = hf_test.map(tokenize, batched=True)\n",
        "test_results = trainer.predict(hf_test_encoded)\n",
        "test_metrics = compute_metrics(test_results)\n",
        "print(\"Test Results:\", test_metrics)\n",
        "\n",
        "# Save predictions\n",
        "y_true = test_results.label_ids\n",
        "y_logits = test_results.predictions\n",
        "y_prob = F.softmax(torch.tensor(y_logits), dim=1).numpy()\n",
        "\n",
        "preds_df = pd.DataFrame({\n",
        "    \"y_true\": y_true,\n",
        "    **{f\"model_1_prob_class_{i}\": y_prob[:, i] for i in range(y_prob.shape[1])}\n",
        "})\n",
        "\n",
        "preds_df.to_csv(\"banglabert_attention_predictions.csv\", index=False)\n",
        "print(\"Predictions saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNNZbpz4Oh5"
      },
      "source": [
        "#XLM roberta with attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqfBZRrm4Rae"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "\n",
        "# ✅ Set Seed for Reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ✅ Define model and tokenizer\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Load dataset from single Excel file\n",
        "file_path = \"/content/BOLT.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# ✅ Convert labels to numerical values\n",
        "label_mapping = {\"no aggression\": 3, \"atrocity\": 2, \"vandalism\": 1, \"hate\": 0}\n",
        "df['label'] = df['Final Annotation'].map(label_mapping)\n",
        "\n",
        "# ✅ Split dataset into Train (80%), Validation (10%), and Test (10%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=seed, stratify=df['label'])\n",
        "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=seed, stratify=temp_df['label'])\n",
        "\n",
        "# ✅ Convert Pandas DataFrame to Hugging Face Dataset format\n",
        "hf_train_dataset = Dataset.from_pandas(train_df)\n",
        "hf_validation_dataset = Dataset.from_pandas(validation_df)\n",
        "hf_test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# ✅ Tokenization function\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['Text'], padding=\"max_length\", max_length=512, truncation=True)\n",
        "\n",
        "# ✅ Create DatasetDict for train & validation\n",
        "data = DatasetDict({'train': hf_train_dataset, 'validation': hf_validation_dataset})\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "data_encoded = data.map(tokenize, batched=True, batch_size=16)\n",
        "\n",
        "# ✅ Define XLM-RoBERTa model with attention and dropout\n",
        "class XLMRobertaWithAttention(nn.Module):\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout_rate = 0.2):\n",
        "        super(XLMRobertaWithAttention, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_name)\n",
        "        for param in self.base_model.parameters():\n",
        "            param.data = param.data.contiguous()\n",
        "        self.base_model.config.hidden_dropout_prob = dropout_rate\n",
        "        self.base_model.config.attention_probs_dropout_prob = dropout_rate\n",
        "        self.hidden_size = self.base_model.config.hidden_size\n",
        "        self.attention = nn.Linear(self.hidden_size, 1).to(torch.float32)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.hidden_size, num_labels)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state.contiguous()\n",
        "        attn_weights = torch.tanh(self.attention(hidden_states))\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
        "        weighted_output = torch.sum(attn_weights * hidden_states, dim=1)\n",
        "        weighted_output = self.dropout(weighted_output.contiguous())\n",
        "        logits = self.classifier(weighted_output)\n",
        "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
        "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
        "\n",
        "# ✅ Define model\n",
        "num_labels = 4\n",
        "dropout_rate = 0.2\n",
        "model = XLMRobertaWithAttention(model_name, num_labels, dropout_rate).to(\"cuda\")\n",
        "\n",
        "# ✅ Define function to compute accuracy and F1-score\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\"Accuracy\": accuracy_score(labels, preds), \"F1 Score\": f1_score(labels, preds, average=\"weighted\")}\n",
        "\n",
        "# ✅ Define training arguments\n",
        "batch_size = 8\n",
        "logging_steps = len(data_encoded[\"train\"]) // batch_size\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-classifier\",         # Output directory\n",
        "    overwrite_output_dir=True,                     # Overwrite if exists\n",
        "    #evaluation_strategy=\"epoch\",   # ✅ REQUIRED to get validation logs\n",
        "    #logging_strategy=\"epoch\",      # ✅ Optional: match logging with eval\n",
        "    do_train=True,                                 # Enable training\n",
        "    do_eval=True,                                  # Enable evaluation\n",
        "    per_device_train_batch_size=8,                 # Batch size for training\n",
        "    per_device_eval_batch_size=8,                  # Batch size for evaluation\n",
        "    num_train_epochs=10,                           # Total number of training epochs\n",
        "    learning_rate=5e-6,                            # Learning rate\n",
        "    weight_decay=0.01,                             # Weight decay\n",
        "    logging_dir=f\"{model_name}-logs\",              # Directory for logs\n",
        "    logging_steps=len(data_encoded['train']) // 8, # Log every N steps\n",
        "    save_steps=len(data_encoded['train']) // 8,    # Save every N steps\n",
        "    save_total_limit=1,                            # Keep only last checkpoint\n",
        "    seed=42,                                        # Reproducibility\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# ✅ Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=data_encoded['train'],\n",
        "    eval_dataset=data_encoded['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# ✅ Train the model with tracking time\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "# ✅ Print overall training time\n",
        "total_time = end_time - start_time\n",
        "print(f\"✅ Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# ✅ Save the trained model\n",
        "model_save_path = \"xlm_roberta_trained_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "tokenizer.save_pretrained(\"xlm_roberta_trained_tokenizer\")\n",
        "print(\"✅ Model saved successfully!\")\n",
        "\n",
        "# ✅ Evaluate on validation set\n",
        "validation_results = trainer.evaluate()\n",
        "print(\"Validation Set Results:\", validation_results)\n",
        "\n",
        "# ✅ Encode the test set before evaluation\n",
        "hf_test_dataset_encoded = hf_test_dataset.map(tokenize, batched=True, batch_size=16)\n",
        "\n",
        "# ✅ Evaluate on test set\n",
        "test_results = trainer.predict(hf_test_dataset_encoded)\n",
        "test_metrics = compute_metrics(test_results)\n",
        "print(\"Test Set Results:\", test_metrics)\n",
        "\n",
        "# ✅ Save True Labels and Predicted Probabilities for Ensemble\n",
        "y_true = test_results.label_ids\n",
        "y_logits = test_results.predictions\n",
        "y_prob = F.softmax(torch.tensor(y_logits), dim=1).numpy()\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"y_true\": y_true,\n",
        "    \"model_1_prob_class_0\": y_prob[:, 0],\n",
        "    \"model_1_prob_class_1\": y_prob[:, 1],\n",
        "    \"model_1_prob_class_2\": y_prob[:, 2],\n",
        "    \"model_1_prob_class_3\": y_prob[:, 3]\n",
        "})\n",
        "\n",
        "results_df.to_csv(\"xlm_roberta_attention_predictions.csv\", index=False)\n",
        "print(\"✅ Predictions saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN9OLEzO4VwA"
      },
      "source": [
        "#Weighted Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnoB0VDM4Z8Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ✅ Load Predictions from Both Models\n",
        "  banglabert_df = pd.read_csv(\"/content/banglabert_attention_predictions.csv\")\n",
        "  xlm_roberta_df = pd.read_csv(\"/content/xlm_roberta_attention_predictions.csv\")\n",
        "\n",
        "# ✅ Ensure both have the same true labels\n",
        "assert np.array_equal(banglabert_df[\"y_true\"], xlm_roberta_df[\"y_true\"]), \"Mismatch in true labels!\"\n",
        "\n",
        "# ✅ Extract True Labels and Probabilities\n",
        "y_true = banglabert_df[\"y_true\"]\n",
        "\n",
        "# Extract probabilities for all classes from both models\n",
        "banglabert_probs = banglabert_df.iloc[:, 1:].values  # Model 1 probabilities\n",
        "xlm_roberta_probs = xlm_roberta_df.iloc[:, 1:].values  # Model 2 probabilities\n",
        "\n",
        "#input test accuracies of the models\n",
        "\n",
        "w_1 = 86.7\n",
        "w_2 = 85.3\n",
        "\n",
        "# ✅ Compute Weighted Ensemble Probabilities\n",
        "ensemble_probs = ((banglabert_probs * w_1) + (xlm_roberta_probs * w_2)) / (w_1 + w_2)\n",
        "\n",
        "# ✅ Compute Final Predictions\n",
        "ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# ✅ Evaluate Performance\n",
        "weighted_ensemble_acc = accuracy_score(y_true, ensemble_preds)\n",
        "weighted_ensemble_f1 = f1_score(y_true, ensemble_preds, average=\"weighted\")\n",
        "print(f\"✅ Weighted Ensemble - Accuracy: {weighted_ensemble_acc:.4f}, F1 Score: {weighted_ensemble_f1:.4f}\")\n",
        "\n",
        "# ✅ Define class labels\n",
        "class_labels = [\"Hate\", \"Vandalism\", \"Atrocity\", \"No Aggression\"]\n",
        "\n",
        "# ✅ Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_true, ensemble_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "\n",
        "# ✅ Plot Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Greens\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix of Weighted Ensemble\")\n",
        "plt.show()\n",
        "\n",
        "# ✅ Generate Classification Report\n",
        "report = classification_report(y_true, ensemble_preds, target_names=class_labels, digits=4)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "\n",
        "# ✅ Compute Accuracy Per Class\n",
        "accuracy_per_class = []\n",
        "for i in range(len(class_labels)):\n",
        "    class_mask = (y_true == i)\n",
        "    class_accuracy = accuracy_score(y_true[class_mask], ensemble_preds[class_mask])\n",
        "    accuracy_per_class.append(class_accuracy)\n",
        "\n",
        "# ✅ Print Accuracy per Class\n",
        "print(\"\\nAccuracy Per Class:\")\n",
        "for label, acc in zip(class_labels, accuracy_per_class):\n",
        "    print(f\"{label}: {acc:.4f}\")\n",
        "\n",
        "# ✅ Compute Overall Performance Metrics\n",
        "precision = precision_score(y_true, ensemble_preds, average='macro')\n",
        "recall = recall_score(y_true, ensemble_preds, average='macro')\n",
        "f1 = f1_score(y_true, ensemble_preds, average='macro')\n",
        "weighted_f1 = f1_score(y_true, ensemble_preds, average='weighted')\n",
        "error_rate = 1 - weighted_ensemble_acc\n",
        "\n",
        "print(f\"\\n✅ Overall Test Accuracy: {weighted_ensemble_acc:.4f}\")\n",
        "print(f\"✅ Overall Test Precision: {precision:.4f}\")\n",
        "print(f\"✅ Overall Test Recall: {recall:.4f}\")\n",
        "print(f\"✅ Overall Test F1 Score: {f1:.4f}\")\n",
        "print(f\"✅ Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"✅ Error Rate: {error_rate:.4f}\")\n",
        "\n",
        "# ✅ Plot ROC Curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "for i in range(len(class_labels)):\n",
        "    fpr, tpr, _ = roc_curve(y_true == i, ensemble_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})', linestyle='-', linewidth=2, color=colors[i])\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Weighted Ensemble\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# ✅ Compute Metrics Per Class\n",
        "precision_per_class = precision_score(y_true, ensemble_preds, average=None)\n",
        "recall_per_class = recall_score(y_true, ensemble_preds, average=None)\n",
        "f1_per_class = f1_score(y_true, ensemble_preds, average=None)\n",
        "\n",
        "# ✅ Compute Macro and Weighted Averages\n",
        "macro_avg = [weighted_ensemble_acc, precision, recall, f1]\n",
        "weighted_avg = [weighted_ensemble_acc, precision_score(y_true, ensemble_preds, average='weighted'), recall_score(y_true, ensemble_preds, average='weighted'), weighted_f1]\n",
        "\n",
        "# ✅ Create and Display 2D Metrics Matrix\n",
        "metrics_df = pd.DataFrame(\n",
        "    data=np.vstack([accuracy_per_class, precision_per_class, recall_per_class, f1_per_class]).T,\n",
        "    index=class_labels,\n",
        "    columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        ")\n",
        "metrics_df.loc[\"M. avg\"] = macro_avg\n",
        "metrics_df.loc[\"W. avg\"] = weighted_avg\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(metrics_df, annot=True, fmt=\".4f\", cmap=\"Greens\", linewidths=0.5)\n",
        "plt.title(\"Performance Metrics per Class for Weighted Ensemble\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
